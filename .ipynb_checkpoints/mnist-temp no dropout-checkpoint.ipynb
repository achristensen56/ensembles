{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we want to assess the difference in noise correlations between models trained with and without dropout.\n",
    "\n",
    "We do that by:\n",
    "\n",
    "1. Training the networks with and without dropouts\n",
    "2. Collecting the embedding layers activations on test data\n",
    "3. Training classifiers on the embedding layers suffled activity, and assessing it unshuffled test data (for both dropout and non-dropout networks)\n",
    "4. Assessing covariance of embedding layer \n",
    "\n",
    "5. Finally we vary the dropout probability and determine noise correlations as a function of dropout probability (or batch size, not sure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make the most vanilla convnet to run MNIST. This is literally the exact conv-net from the 'deep mnist for experts' tutorial. \n",
    "\n",
    "We can specify at training time the keep probability, which when we want to set a network with no dropout we just set to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class convNet():\n",
    "    def __init__(self):\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        self.sess = tf.Session()\n",
    "        self.x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "        self.y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "        self.W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "        self.b_conv1 = bias_variable([32])\n",
    "\n",
    "        self.x_image = tf.reshape(self.x, [-1,28,28,1])\n",
    "\n",
    "        self.h_conv1 = tf.nn.relu(conv2d(self.x_image, self.W_conv1) + self.b_conv1)\n",
    "        self.h_pool1 = max_pool_2x2(self.h_conv1)\n",
    "\n",
    "        self.W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "        self.b_conv2 = bias_variable([64])\n",
    "\n",
    "        self.h_conv2 = tf.nn.relu(conv2d(self.h_pool1, self.W_conv2) + self.b_conv2)\n",
    "        self.h_pool2 = max_pool_2x2(self.h_conv2)\n",
    "\n",
    "        self.W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "        self.b_fc1 = bias_variable([1024])\n",
    "\n",
    "        self.h_pool2_flat = tf.reshape(self.h_pool2, [-1, 7*7*64])\n",
    "        self.h_fc1 = tf.nn.relu(tf.matmul(self.h_pool2_flat, self.W_fc1) + self.b_fc1)\n",
    "\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        self.h_fc1_drop = tf.nn.dropout(self.h_fc1, self.keep_prob)\n",
    "\n",
    "        self.W_fc2 = weight_variable([1024, 10])\n",
    "        self.b_fc2 = bias_variable([10])\n",
    "\n",
    "        self.y_conv = tf.matmul(self.h_fc1_drop, self.W_fc2) + self.b_fc2\n",
    "        \n",
    "        self.cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(self.y_conv, self.y_))\n",
    "        self.train_step = tf.train.AdamOptimizer(1e-4).minimize(self.cross_entropy)\n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.y_conv,1), tf.argmax(self.y_,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "        self.sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "    def fit(self, max_iters = 2000, epoch_size = 100, batch_size = 50, train_keep_prob = .5):\n",
    "        for i in range(max_iters):\n",
    "            batch = mnist.train.next_batch(batch_size)\n",
    "            if i%epoch_size == 0:\n",
    "                train_accuracy = self.accuracy.eval(session = self.sess, feed_dict={\n",
    "                    self.x:batch[0], self.y_: batch[1], self.keep_prob: 1.0})\n",
    "                print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "            \n",
    "            self.sess.run(self.train_step, feed_dict={\n",
    "                self.x: batch[0], self.y_: batch[1], self.keep_prob: train_keep_prob})\n",
    "\n",
    "        print(\"test accuracy %g\"%self.accuracy.eval(session = self.sess, feed_dict={\n",
    "            self.x: mnist.test.images, self.y_: mnist.test.labels, selfkeep_prob: 1.0}))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.12\n",
      "step 100, training accuracy 0.885\n",
      "step 200, training accuracy 0.9\n",
      "step 300, training accuracy 0.935\n",
      "step 400, training accuracy 0.945\n",
      "step 500, training accuracy 0.96\n",
      "step 600, training accuracy 0.965\n",
      "step 700, training accuracy 0.975\n",
      "step 800, training accuracy 0.97\n",
      "step 900, training accuracy 0.965\n",
      "step 1000, training accuracy 0.96\n",
      "step 1100, training accuracy 0.975\n",
      "step 1200, training accuracy 0.98\n"
     ]
    }
   ],
   "source": [
    "drop_net = convNet()\n",
    "drop_net.fit(max_iters = 5000, batch_size = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#trials = [sess.run(h_fc1_drop, feed_dict = {x: mnist.test.images[0:100], keep_prob: .5}) for i in range(50)]\n",
    "#on a small amazon instance can't do more than 10000 training examples. Will fix this tmrw once the code is written. \n",
    "\n",
    "all_units = sess.run(h_fc1_drop, feed_dict = {x: mnist.test.images[0:1000], keep_prob: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ylabels = [np.argmax(row) for row in mnist.test.labels[0:1000]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96399999999999997"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#ylabels = LabelEncoder().fit_transform(mnist.test.labels[0:1000])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_units, ylabels)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tensorize(data, labels, est_trials = 100, shuffle = False):\n",
    "    n_points, n_neurons = data.shape\n",
    "    n_classes = len(set(labels))\n",
    "    \n",
    "    tensor = np.empty([n_classes, est_trials, n_neurons])\n",
    "    trial_count = np.zeros(n_classes).astype('int')\n",
    "    \n",
    "    for i in range(n_points):\n",
    "        \n",
    "        tensor[labels[i]][trial_count[labels[i]]] = data[i]\n",
    "        trial_count[labels[i]] += 1\n",
    "        \n",
    "    num_trials = min(trial_count).astype('int')\n",
    "    tensor = tensor[:, 0:num_trials,: ]\n",
    "    \n",
    "    if shuffle:\n",
    "        \n",
    "        for i in range(n_neurons):\n",
    "            for j in range(n_classes):      \n",
    "                tensor[j, :, i] = np.random.permutation(tensor[j, :, i])\n",
    "\n",
    "    labels = [range(10) for i in range(num_trials)]\n",
    "    labels = np.array(train_labels).transpose()\n",
    "    \n",
    "    return tensor, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_tensor, train_labels = tensorize(X_train, y_train, shuffle = True)\n",
    "test_tensor, test_labels = tensorize(X_test, y_test, shuffle = False)\n",
    "\n",
    "train_tensor, train_labels = train_tensor.reshape(-1, 1024), train_labels.reshape([-1])\n",
    "test_tensor, test_labels = test_tensor.reshape(-1, 1024), test_labels.reshape([-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93600000000000005"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(train_tensor, train_labels)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
